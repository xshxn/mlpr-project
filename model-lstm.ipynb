{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5bab19",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-05T09:50:28.856296Z",
     "iopub.status.busy": "2025-05-05T09:50:28.856042Z",
     "iopub.status.idle": "2025-05-05T09:50:42.221514Z",
     "shell.execute_reply": "2025-05-05T09:50:42.220756Z"
    },
    "papermill": {
     "duration": 13.370705,
     "end_time": "2025-05-05T09:50:42.222991",
     "exception": false,
     "start_time": "2025-05-05T09:50:28.852286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\r\n",
      "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\r\n",
      "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\r\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\r\n",
      "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\n",
      "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\r\n",
      "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\r\n",
      "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.13.0\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, Subset\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from transformers.feature_extraction_utils import BatchFeature\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "!pip install python-Levenshtein\n",
    "import Levenshtein\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac035b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:50:42.230033Z",
     "iopub.status.busy": "2025-05-05T09:50:42.229096Z",
     "iopub.status.idle": "2025-05-05T09:50:42.445232Z",
     "shell.execute_reply": "2025-05-05T09:50:42.444456Z"
    },
    "papermill": {
     "duration": 0.220768,
     "end_time": "2025-05-05T09:50:42.446642",
     "exception": false,
     "start_time": "2025-05-05T09:50:42.225874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/mlpr-data/torgo_vectors_transcripts.csv')\n",
    "df['FeaturePath'] = df['FeaturePath'].str.replace(\"E:\\\\MLPR Data\\\\Features\\\\\", \"/kaggle/input/mlpr-data/Features/Features/\")\n",
    "\n",
    "# Create mask to filter out entries with 'input' and 'jpg'\n",
    "mask1 = ~(df['transcipt'].str.contains('input', case=False, na=False) & \n",
    "         df['transcipt'].str.contains('jpg', case=False, na=False))\n",
    "\n",
    "# Create mask to filter out entries with 'say' and 'repeatedly'\n",
    "mask2 = ~(df['transcipt'].str.contains('say', case=False, na=False) & \n",
    "         df['transcipt'].str.contains('repeatedly', case=False, na=False))\n",
    "\n",
    "# Combine both masks\n",
    "mask = mask1 & mask2\n",
    "\n",
    "df = df[mask]\n",
    "speakers = df[\"Speaker\"].unique() \n",
    "df.to_csv('mlpr-torgo-kaggle.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a016fec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:50:42.452629Z",
     "iopub.status.busy": "2025-05-05T09:50:42.452381Z",
     "iopub.status.idle": "2025-05-05T09:50:42.459152Z",
     "shell.execute_reply": "2025-05-05T09:50:42.458462Z"
    },
    "papermill": {
     "duration": 0.010982,
     "end_time": "2025-05-05T09:50:42.460270",
     "exception": false,
     "start_time": "2025-05-05T09:50:42.449288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TorgoASRDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        feature_path = row[\"FeaturePath\"]\n",
    "        transcript = row[\"transcipt\"]\n",
    "        speaker = row[\"Speaker\"]\n",
    "    \n",
    "        try:\n",
    "            features = torch.load(feature_path, map_location='cpu')\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load feature from '{feature_path}': {e}\")\n",
    "        \n",
    "        if isinstance(features, dict):\n",
    "            input_values = features.get(\"input_values\")\n",
    "            if input_values is None:\n",
    "                raise ValueError(f\"'input_values' key not found in features loaded from {feature_path}\")\n",
    "        elif hasattr(features, \"input_values\"):\n",
    "            input_values = features.input_values\n",
    "        else:\n",
    "            input_values = features\n",
    "    \n",
    "        if not isinstance(input_values, torch.Tensor):\n",
    "            input_values = torch.tensor(input_values)\n",
    "    \n",
    "        if input_values.dim() == 3:\n",
    "            input_values = input_values.squeeze(0)  # now shape is (T, hidden_size)\n",
    "    \n",
    "        seq_length = input_values.size(0)\n",
    "        \n",
    "        return {\n",
    "            \"input_values\": input_values,\n",
    "            \"seq_length\": seq_length,\n",
    "            \"transcript\": transcript,\n",
    "            \"speaker\": speaker\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97e58a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:50:42.465691Z",
     "iopub.status.busy": "2025-05-05T09:50:42.465478Z",
     "iopub.status.idle": "2025-05-05T09:50:42.470518Z",
     "shell.execute_reply": "2025-05-05T09:50:42.470027Z"
    },
    "papermill": {
     "duration": 0.008927,
     "end_time": "2025-05-05T09:50:42.471556",
     "exception": false,
     "start_time": "2025-05-05T09:50:42.462629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_values_list = []\n",
    "    seq_lengths = []\n",
    "    transcripts = []\n",
    "    speakers = []\n",
    "    \n",
    "    for sample in batch:\n",
    "        \n",
    "        x = sample[\"input_values\"]\n",
    "        sample_seq_length = x.size(0)\n",
    "        \n",
    "        input_values_list.append(x)\n",
    "        seq_lengths.append(sample_seq_length)\n",
    "        transcripts.append(sample[\"transcript\"])\n",
    "        speakers.append(sample[\"speaker\"])\n",
    "    \n",
    "\n",
    "    padded_inputs = torch.nn.utils.rnn.pad_sequence(input_values_list, batch_first=True, padding_value=0)\n",
    "    \n",
    "    \n",
    "    padded_inputs = padded_inputs.contiguous()\n",
    "    \n",
    "    return {\n",
    "        \"input_values\": padded_inputs,  # Now shape: (batch, time, hidden_size)\n",
    "        \"seq_lengths\": torch.tensor(seq_lengths),\n",
    "        \"transcripts\": transcripts,\n",
    "        \"speakers\": speakers\n",
    "    }\n",
    "\n",
    "\n",
    "def transcript_to_indices(transcript, char_to_idx):\n",
    "    return [char_to_idx[char] for char in transcript if char in char_to_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1e651",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:50:42.477249Z",
     "iopub.status.busy": "2025-05-05T09:50:42.477035Z",
     "iopub.status.idle": "2025-05-05T09:50:42.487665Z",
     "shell.execute_reply": "2025-05-05T09:50:42.487134Z"
    },
    "papermill": {
     "duration": 0.014707,
     "end_time": "2025-05-05T09:50:42.488623",
     "exception": false,
     "start_time": "2025-05-05T09:50:42.473916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=2, dropout_rate=0.3):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM layers with layer normalization\n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "        self.layer_norms = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            input_size = hidden_dim if i == 0 else hidden_dim * 2\n",
    "            self.lstm_layers.append(nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_dim,\n",
    "                batch_first=True,\n",
    "                bidirectional=True\n",
    "            ))\n",
    "            self.layer_norms.append(nn.LayerNorm(hidden_dim * 2))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def forward(self, x, input_lengths=None):\n",
    "        # Project input to hidden dimension\n",
    "        if x.dim() == 3 and x.size(2) == 1:  \n",
    "            x = x.squeeze(2)\n",
    "            x = x.unsqueeze(2)\n",
    "        \n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Process through LSTM layers with layer normalization and residual connections\n",
    "        for i, (lstm, norm) in enumerate(zip(self.lstm_layers, self.layer_norms)):\n",
    "            residual = x if i > 0 and x.size(-1) == hidden_dim * 2 else None\n",
    "            \n",
    "            # Use packed sequence for variable-length inputs\n",
    "            if input_lengths is not None and i == 0:\n",
    "                packed_x = rnn_utils.pack_padded_sequence(x, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "                packed_output, (h, c) = lstm(packed_x)\n",
    "                lstm_out, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n",
    "            else:\n",
    "                lstm_out, (h, c) = lstm(x)\n",
    "            \n",
    "            lstm_out = self.dropout(lstm_out)\n",
    "            lstm_out = norm(lstm_out)\n",
    "            \n",
    "            # Add residual connection if dimensions match\n",
    "            if residual is not None:\n",
    "                lstm_out = lstm_out + residual\n",
    "            \n",
    "            x = lstm_out\n",
    "            \n",
    "            # Save final states for last layer\n",
    "            if i == self.num_layers - 1:\n",
    "                final_h, final_c = h, c\n",
    "        \n",
    "        # Process final states for decoder initialization\n",
    "        forward_h = final_h[0].unsqueeze(0)   # [1, batch_size, hidden_dim]\n",
    "        backward_h = final_h[1].unsqueeze(0)  # [1, batch_size, hidden_dim]\n",
    "        forward_c = final_c[0].unsqueeze(0)   # [1, batch_size, hidden_dim]\n",
    "        backward_c = final_c[1].unsqueeze(0)  # [1, batch_size, hidden_dim]\n",
    "        \n",
    "        # Concatenate bidirectional states\n",
    "        h_concat = torch.cat([forward_h, backward_h], dim=2)  # [1, batch_size, hidden_dim*2]\n",
    "        c_concat = torch.cat([forward_c, backward_c], dim=2)  # [1, batch_size, hidden_dim*2]\n",
    "        \n",
    "        return x, (h_concat, c_concat)  # encoder_outputs, (hidden, cell)\n",
    "\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau (additive) attention as described in the paper:\"\"\"\n",
    "    \"\"\"'Neural Machine Translation by Jointly Learning to Align and Translate' (2015)\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_dim, decoder_dim):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        # Alignment model (Bahdanau's additive attention)\n",
    "        self.W_a = nn.Linear(encoder_dim, decoder_dim, bias=False)\n",
    "        self.U_a = nn.Linear(decoder_dim, decoder_dim, bias=False)\n",
    "        self.v_a = nn.Linear(decoder_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        \"\"\"Calculate context vector using Bahdanau attention mechanism\n",
    "        \n",
    "        Args:\n",
    "            hidden: decoder's hidden state [1, batch_size, decoder_dim]\n",
    "            encoder_outputs: outputs from encoder [batch_size, src_len, encoder_dim]\n",
    "            mask: mask for padded elements in encoder_outputs, if any\n",
    "            \n",
    "        Returns:\n",
    "            context: context vector [batch_size, encoder_dim]\n",
    "            attention_weights: attention weights [batch_size, src_len]\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        src_len = encoder_outputs.size(1)\n",
    "        \n",
    "        # Reshape decoder hidden state to match batch dimension\n",
    "        hidden = hidden.transpose(0, 1).contiguous()  # [batch_size, 1, decoder_dim]\n",
    "        hidden_expanded = hidden.repeat(1, src_len, 1)  # [batch_size, src_len, decoder_dim]\n",
    "        \n",
    "        # Calculate alignment scores\n",
    "        # First transform encoder outputs with W_a\n",
    "        encoder_transform = self.W_a(encoder_outputs)  # [batch_size, src_len, decoder_dim]\n",
    "        \n",
    "        # Then transform hidden state with U_a\n",
    "        hidden_transform = self.U_a(hidden.squeeze(1)).unsqueeze(1)  # [batch_size, 1, decoder_dim]\n",
    "        \n",
    "        # Combine transforms and apply tanh\n",
    "        # score = tanh(W_a*h_enc + U_a*h_dec)\n",
    "        energy = torch.tanh(encoder_transform + hidden_transform)  # [batch_size, src_len, decoder_dim]\n",
    "        \n",
    "        # Apply v_a to get scalar scores\n",
    "        energy = self.v_a(energy).squeeze(2)  # [batch_size, src_len]\n",
    "        \n",
    "        # Apply mask if provided (for padding)\n",
    "        if mask is not None:\n",
    "            energy.masked_fill_(mask == 0, -1e10)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(energy, dim=1)  # [batch_size, src_len]\n",
    "        \n",
    "        # Apply attention weights to get context vector\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # [batch_size, 1, encoder_dim]\n",
    "        context = context.squeeze(1)  # [batch_size, encoder_dim]\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_dim, hidden_dim, dropout_rate=0.3):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Character embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Initialize decoder state with encoder final state\n",
    "        self.init_h = nn.Linear(encoder_dim, hidden_dim)\n",
    "        self.init_c = nn.Linear(encoder_dim, hidden_dim)\n",
    "        \n",
    "        # Bahdanau attention mechanism\n",
    "        self.attention = BahdanauAttention(encoder_dim, hidden_dim)\n",
    "        \n",
    "        # LSTM for sequence processing\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim + encoder_dim,  # Input: embedding + context\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Output projection with residual connection\n",
    "        self.character_projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + encoder_dim + embedding_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, vocab_size)\n",
    "        )\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder_dim = encoder_dim\n",
    "        \n",
    "    def init_hidden(self, encoder_hidden, encoder_cell):\n",
    "        \"\"\"Initialize decoder state based on encoder final state\"\"\"\n",
    "        # encoder_hidden/cell shape: [1, batch_size, encoder_dim]\n",
    "        h = self.init_h(encoder_hidden)\n",
    "        c = self.init_c(encoder_cell)\n",
    "        return h, c\n",
    "        \n",
    "    def forward(self, input_char, hidden, cell, encoder_outputs, encoder_mask=None):\n",
    "        \"\"\"Single step of decoder with Bahdanau attention\"\"\"\n",
    "        # Embed current character\n",
    "        embedded = self.embedding(input_char).unsqueeze(1)  # [batch_size, 1, embedding_dim]\n",
    "        \n",
    "        # Calculate attention and context vector\n",
    "        context, attn_weights = self.attention(hidden, encoder_outputs, encoder_mask)\n",
    "        context = context.unsqueeze(1)  # [batch_size, 1, encoder_dim]\n",
    "        \n",
    "        # Concatenate embedding and context for LSTM input\n",
    "        lstm_input = torch.cat((embedded, context), dim=2)  # [batch_size, 1, embedding_dim + encoder_dim]\n",
    "        \n",
    "        # Process through LSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        \n",
    "        # Prepare output projection input by concatenating all available information\n",
    "        concat_input = torch.cat((lstm_out.squeeze(1), context.squeeze(1), embedded.squeeze(1)), dim=1)\n",
    "        output = self.character_projection(concat_input)\n",
    "        \n",
    "        return output, hidden, cell, attn_weights\n",
    "\n",
    "\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size, sos_idx=0, eos_idx=0, pad_idx=0,\n",
    "                 embedding_dim=128, encoder_layers=2, dropout_rate=0.3, max_decoding_length=150, device='cpu'):\n",
    "        super(EncoderDecoderModel, self).__init__()\n",
    "        \n",
    "        # Encoder with multiple BiLSTM layers\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, num_layers=encoder_layers, dropout_rate=dropout_rate)\n",
    "        \n",
    "        # Decoder with Bahdanau attention\n",
    "        encoder_output_dim = hidden_dim * 2  # BiLSTM outputs twice the hidden dim\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, encoder_output_dim, hidden_dim, dropout_rate)\n",
    "        \n",
    "        # Special tokens\n",
    "        self.sos_idx = sos_idx  # Start of sequence token\n",
    "        self.eos_idx = eos_idx  # End of sequence token\n",
    "        self.pad_idx = pad_idx  # Padding token\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_decoding_length = max_decoding_length\n",
    "        \n",
    "    def forward(self, x, target_sequence=None, teacher_forcing_ratio=0.5, max_length=100):\n",
    "        \"\"\"Forward pass with teacher forcing for training\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Encode input sequence\n",
    "        encoder_outputs, (encoder_hidden, encoder_cell) = self.encoder(x)\n",
    "        \n",
    "        # Create encoder mask for attention (1 for real tokens, 0 for padding)\n",
    "        encoder_mask = (torch.sum(encoder_outputs, dim=2) != 0).float().unsqueeze(1)\n",
    "        \n",
    "        # Initialize decoder state\n",
    "        hidden, cell = self.decoder.init_hidden(\n",
    "            encoder_hidden.transpose(0, 1).contiguous().transpose(1, 2).contiguous().transpose(1, 0),\n",
    "            encoder_cell.transpose(0, 1).contiguous().transpose(1, 2).contiguous().transpose(1, 0)\n",
    "        )\n",
    "        \n",
    "        # Set maximum decoding length\n",
    "        if target_sequence is not None:\n",
    "            max_length = min(target_sequence.size(1), self.max_decoding_length)\n",
    "        else:\n",
    "            max_length = min(self.max_decoding_length, max_length)\n",
    "        \n",
    "        # Initial decoder input: start token\n",
    "        decoder_input = torch.ones(batch_size, dtype=torch.long).to(self.device) * self.sos_idx\n",
    "        \n",
    "        # Tensors to store outputs and attention\n",
    "        outputs = torch.zeros(batch_size, max_length, self.vocab_size).to(self.device)\n",
    "        attentions = torch.zeros(batch_size, max_length, encoder_outputs.size(1)).to(self.device)\n",
    "        \n",
    "        # Autoregressive decoding with teacher forcing\n",
    "        for t in range(max_length):\n",
    "            output, hidden, cell, attn_weights = self.decoder(\n",
    "                decoder_input, hidden, cell, encoder_outputs, encoder_mask\n",
    "            )\n",
    "            \n",
    "            # Store output and attention\n",
    "            outputs[:, t] = output\n",
    "            attentions[:, t] = attn_weights\n",
    "            \n",
    "            # Teacher forcing: use ground truth vs. previous prediction\n",
    "            if target_sequence is not None and t < max_length - 1 and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                # Use ground truth for next input\n",
    "                decoder_input = target_sequence[:, t]\n",
    "            else:\n",
    "                # Use model prediction for next input\n",
    "                top1 = output.argmax(1)\n",
    "                decoder_input = top1\n",
    "        \n",
    "        return outputs, attentions\n",
    "    \n",
    "    def decode(self, x, max_length=None):\n",
    "        \"\"\"Generate text prediction using greedy decoding\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        if max_length is None:\n",
    "            max_length = self.max_decoding_length\n",
    "        else:\n",
    "            max_length = min(self.max_decoding_length, max_length)\n",
    "        \n",
    "        # Encode input sequence\n",
    "        encoder_outputs, (encoder_hidden, encoder_cell) = self.encoder(x)\n",
    "        \n",
    "        # Create encoder mask for attention\n",
    "        encoder_mask = (torch.sum(encoder_outputs, dim=2) != 0).float().unsqueeze(1)\n",
    "        \n",
    "        # Initialize decoder state\n",
    "        hidden, cell = self.decoder.init_hidden(\n",
    "            encoder_hidden.transpose(0, 1).contiguous().transpose(1, 2).contiguous().transpose(1, 0),\n",
    "            encoder_cell.transpose(0, 1).contiguous().transpose(1, 2).contiguous().transpose(1, 0)\n",
    "        )\n",
    "        \n",
    "        # Initial decoder input: start token\n",
    "        decoder_input = torch.ones(batch_size, dtype=torch.long).to(self.device) * self.sos_idx\n",
    "        \n",
    "        # Lists to store predictions\n",
    "        all_predictions = []\n",
    "        has_ended = torch.zeros(batch_size, dtype=torch.bool).to(self.device)\n",
    "        \n",
    "        # Autoregressive greedy decoding\n",
    "        for t in range(max_length):\n",
    "            output, hidden, cell, _ = self.decoder(\n",
    "                decoder_input, hidden, cell, encoder_outputs, encoder_mask\n",
    "            )\n",
    "            \n",
    "            # Greedy selection of most likely character\n",
    "            predictions = output.argmax(1)\n",
    "            all_predictions.append(predictions.unsqueeze(1))\n",
    "            \n",
    "            # Check for end of sequence token\n",
    "            has_ended = has_ended | (predictions == self.eos_idx)\n",
    "            if has_ended.all():\n",
    "                break\n",
    "                \n",
    "            # Use current prediction as next input\n",
    "            decoder_input = predictions\n",
    "        \n",
    "        # Concatenate all predictions\n",
    "        predictions = torch.cat(all_predictions, dim=1)  # [batch_size, seq_len]\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ea160",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:50:42.494217Z",
     "iopub.status.busy": "2025-05-05T09:50:42.493998Z",
     "iopub.status.idle": "2025-05-05T09:50:42.516468Z",
     "shell.execute_reply": "2025-05-05T09:50:42.515739Z"
    },
    "papermill": {
     "duration": 0.026661,
     "end_time": "2025-05-05T09:50:42.517617",
     "exception": false,
     "start_time": "2025-05-05T09:50:42.490956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_cer(reference, prediction):\n",
    "    distance = Levenshtein.distance(reference, prediction)\n",
    "    return distance / max(len(reference), 1)\n",
    "\n",
    "def calculate_wer(reference, prediction):\n",
    "    ref_words = reference.split()\n",
    "    pred_words = prediction.split()\n",
    "    distance = Levenshtein.distance(ref_words, pred_words)\n",
    "    return distance / max(len(ref_words), 1)\n",
    "\n",
    "def preprocess_targets(transcripts, char_to_idx):\n",
    "    \"\"\"Convert text transcripts to index sequences and pad them\"\"\"\n",
    "    target_seqs = [torch.tensor([char_to_idx.get(c, 0) for c in t if c in char_to_idx], dtype=torch.long) \n",
    "                  for t in transcripts]\n",
    "    \n",
    "    # Make sure all sequences have at least one token\n",
    "    target_seqs = [t if len(t) > 0 else torch.tensor([0], dtype=torch.long) for t in target_seqs]\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_targets = rnn_utils.pad_sequence(target_seqs, batch_first=True, padding_value=0)\n",
    "    return padded_targets\n",
    "\n",
    "def trainModel(model, train_loader, val_loader, char_to_idx, num_epochs=10, learning_rate=1e-4, patience=3, min_delta=0.001, teacher_forcing_ratio=0.5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "    model.device = device  # Update model's device attribute\n",
    "    \n",
    "    # Use cross entropy loss (handles the logits and target indices)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        \n",
    "        for batch in train_pbar:\n",
    "            inputs = batch[\"input_values\"].to(device)\n",
    "            transcripts = batch[\"transcripts\"]\n",
    "            \n",
    "            # Preprocess target sequences\n",
    "            target_seqs = preprocess_targets(transcripts, char_to_idx).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass with teacher forcing\n",
    "            outputs, _ = model(inputs, target_seqs, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "            \n",
    "            # Reshape outputs for CrossEntropyLoss\n",
    "            batch_size, seq_len, vocab_size = outputs.size()\n",
    "            outputs_flat = outputs.view(-1, vocab_size)\n",
    "            targets_flat = target_seqs.view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs_flat, targets_flat)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            train_pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_pbar:\n",
    "                inputs = batch[\"input_values\"].to(device)\n",
    "                transcripts = batch[\"transcripts\"]\n",
    "                \n",
    "                # Preprocess target sequences\n",
    "                target_seqs = preprocess_targets(transcripts, char_to_idx).to(device)\n",
    "                \n",
    "                # Forward pass without teacher forcing\n",
    "                outputs, _ = model(inputs, target_seqs, teacher_forcing_ratio=0.0)\n",
    "                \n",
    "                # Reshape outputs for CrossEntropyLoss\n",
    "                batch_size, seq_len, vocab_size = outputs.size()\n",
    "                outputs_flat = outputs.view(-1, vocab_size)\n",
    "                targets_flat = target_seqs.view(-1)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs_flat, targets_flat)\n",
    "                val_loss += loss.item()\n",
    "                val_pbar.set_postfix({\"val_loss\": f\"{loss.item():.4f}\"})\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current learning rate: {current_lr:.6f}\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"New best validation loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement for {epochs_without_improvement} epochs\")\n",
    "            \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "    \n",
    "    if best_model_state is not None and epochs_without_improvement < patience:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "    print(\"Training complete.\")\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "def evaluateModel(model, test_loader, char_to_idx, idx_to_char, output_csv=\"evaluation_results.csv\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.device = device  # Update model's device attribute\n",
    "    model.eval()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
    "    test_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_transcripts = []\n",
    "    all_speakers = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(test_loader, desc=\"Evaluating\")\n",
    "        for batch in test_pbar:\n",
    "            inputs = batch[\"input_values\"].to(device)\n",
    "            transcripts = batch[\"transcripts\"]\n",
    "            speakers = batch[\"speakers\"]\n",
    "            \n",
    "            # Get target sequences for loss calculation\n",
    "            target_seqs = preprocess_targets(transcripts, char_to_idx).to(device)\n",
    "            \n",
    "            # Generate predictions with improved decoder (limit to 50 chars to prevent long sequences)\n",
    "            predictions = model.decode(inputs, max_length=50)\n",
    "            \n",
    "            # Calculate loss with teacher forcing outputs\n",
    "            outputs, _ = model(inputs, target_seqs, teacher_forcing_ratio=0.0)\n",
    "            outputs_flat = outputs.view(-1, model.vocab_size)\n",
    "            targets_flat = target_seqs.view(-1)\n",
    "            loss = criterion(outputs_flat, targets_flat)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Convert prediction indices to text\n",
    "            batch_texts = []\n",
    "            for pred in predictions:\n",
    "                text = ''.join([idx_to_char.get(p.item(), '') for p in pred if p.item() > 0])\n",
    "                # Simple truncation at first double space if it exists\n",
    "                if '  ' in text:\n",
    "                    text = text.split('  ')[0]\n",
    "                batch_texts.append(text)\n",
    "            \n",
    "            all_predictions.extend(batch_texts)\n",
    "            all_transcripts.extend(transcripts)\n",
    "            all_speakers.extend(speakers)\n",
    "            \n",
    "            test_pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        print(f\"Final Test Loss: {avg_test_loss:.4f}\")\n",
    "        \n",
    "        # Calculate overall WER and CER\n",
    "        total_cer = 0.0\n",
    "        total_wer = 0.0\n",
    "        for ref, pred in zip(all_transcripts, all_predictions):\n",
    "            total_cer += calculate_cer(ref, pred)\n",
    "            total_wer += calculate_wer(ref, pred)\n",
    "        \n",
    "        avg_cer = total_cer / len(all_predictions)\n",
    "        avg_wer = total_wer / len(all_predictions)\n",
    "        print(f\"Overall Character Error Rate: {avg_cer:.4f}\")\n",
    "        print(f\"Overall Word Error Rate: {avg_wer:.4f}\")\n",
    "        \n",
    "        # Calculate per-speaker metrics\n",
    "        speaker_predictions = defaultdict(list)\n",
    "        speaker_references = defaultdict(list)\n",
    "        \n",
    "        for speaker, ref, pred in zip(all_speakers, all_transcripts, all_predictions):\n",
    "            speaker_predictions[speaker].append(pred)\n",
    "            speaker_references[speaker].append(ref)\n",
    "        \n",
    "        print(\"\\nPer-Speaker Metrics:\")\n",
    "        for speaker in sorted(speaker_predictions.keys()):\n",
    "            preds = speaker_predictions[speaker]\n",
    "            refs = speaker_references[speaker]\n",
    "            \n",
    "            speaker_cer = sum(calculate_cer(r, p) for r, p in zip(refs, preds)) / len(preds)\n",
    "            speaker_wer = sum(calculate_wer(r, p) for r, p in zip(refs, preds)) / len(preds)\n",
    "            \n",
    "            print(f\"Speaker {speaker} (samples: {len(preds)})\")\n",
    "            print(f\"  - Character Error Rate: {speaker_cer:.4f}\")\n",
    "            print(f\"  - Word Error Rate: {speaker_wer:.4f}\")\n",
    "        \n",
    "        # Print examples\n",
    "        for i in range(min(15, len(all_predictions))):\n",
    "            print(f\"Example {i+1} (Speaker: {all_speakers[i]}):\\nReference: '{all_transcripts[i]}'\\nPrediction: '{all_predictions[i]}'\")\n",
    "        \n",
    "        # Save results to CSV file\n",
    "        results_df = pd.DataFrame({\n",
    "            'speaker': all_speakers,\n",
    "            'reference': all_transcripts,\n",
    "            'prediction': all_predictions,\n",
    "            'cer': [calculate_cer(ref, pred) for ref, pred in zip(all_transcripts, all_predictions)],\n",
    "            'wer': [calculate_wer(ref, pred) for ref, pred in zip(all_transcripts, all_predictions)]\n",
    "        })\n",
    "        \n",
    "        results_df.to_csv(output_csv, index=False)\n",
    "        print(f\"\\nEvaluation results saved to {output_csv}\")\n",
    "        \n",
    "        return avg_test_loss, all_predictions, all_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d1c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T09:50:42.522922Z",
     "iopub.status.busy": "2025-05-05T09:50:42.522711Z",
     "iopub.status.idle": "2025-05-05T10:05:45.061919Z",
     "shell.execute_reply": "2025-05-05T10:05:45.060643Z"
    },
    "papermill": {
     "duration": 902.543218,
     "end_time": "2025-05-05T10:05:45.063097",
     "exception": false,
     "start_time": "2025-05-05T09:50:42.519879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]:   0%|          | 0/81 [00:00<?, ?it/s]/tmp/ipykernel_19/4123727947.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  features = torch.load(feature_path, map_location='cpu')\n",
      "Epoch 1/30 [Train]: 100%|██████████| 81/81 [01:14<00:00,  1.09it/s, loss=3.4227]\n",
      "Epoch 1/30 [Val]: 100%|██████████| 27/27 [00:17<00:00,  1.54it/s, val_loss=3.2810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] Training Loss: 4.0460, Validation Loss: 3.3246\n",
      "Current learning rate: 0.000500\n",
      "New best validation loss: 3.3246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|██████████| 81/81 [00:40<00:00,  2.00it/s, loss=2.1067]\n",
      "Epoch 2/30 [Val]: 100%|██████████| 27/27 [00:08<00:00,  3.36it/s, val_loss=1.4831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30] Training Loss: 2.9373, Validation Loss: 1.6671\n",
      "Current learning rate: 0.000500\n",
      "New best validation loss: 1.6671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|██████████| 81/81 [00:41<00:00,  1.95it/s, loss=1.3185]\n",
      "Epoch 3/30 [Val]: 100%|██████████| 27/27 [00:07<00:00,  3.39it/s, val_loss=1.1698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30] Training Loss: 1.4258, Validation Loss: 1.3569\n",
      "Current learning rate: 0.000500\n",
      "New best validation loss: 1.3569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|██████████| 81/81 [00:41<00:00,  1.97it/s, loss=1.1889]\n",
      "Epoch 4/30 [Val]: 100%|██████████| 27/27 [00:07<00:00,  3.43it/s, val_loss=1.0578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30] Training Loss: 1.2476, Validation Loss: 1.2822\n",
      "Current learning rate: 0.000500\n",
      "New best validation loss: 1.2822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|██████████| 81/81 [00:40<00:00,  2.00it/s, loss=1.3773]\n",
      "Epoch 5/30 [Val]: 100%|██████████| 27/27 [00:07<00:00,  3.49it/s, val_loss=1.0783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30] Training Loss: 1.1790, Validation Loss: 1.2759\n",
      "Current learning rate: 0.000500\n",
      "No improvement for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|██████████| 81/81 [00:40<00:00,  1.98it/s, loss=1.0186]\n",
      "Epoch 6/30 [Val]: 100%|██████████| 27/27 [00:07<00:00,  3.46it/s, val_loss=1.0925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30] Training Loss: 1.1209, Validation Loss: 1.2970\n",
      "Current learning rate: 0.000500\n",
      "No improvement for 2 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|██████████| 81/81 [00:39<00:00,  2.03it/s, loss=1.3075]\n",
      "Epoch 7/30 [Val]: 100%|██████████| 27/27 [00:07<00:00,  3.48it/s, val_loss=1.0583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30] Training Loss: 1.0674, Validation Loss: 1.2332\n",
      "Current learning rate: 0.000500\n",
      "New best validation loss: 1.2332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|██████████| 81/81 [00:41<00:00,  1.96it/s, loss=1.2491]\n",
      "Epoch 8/30 [Val]: 100%|██████████| 27/27 [00:07<00:00,  3.45it/s, val_loss=1.0402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30] Training Loss: 1.0282, Validation Loss: 1.2714\n",
      "Current learning rate: 0.000500\n",
      "No improvement for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Train]: 100%|██████████| 81/81 [00:40<00:00,  2.02it/s, loss=1.1432]\n",
      "Epoch 9/30 [Val]: 100%|██████████| 27/27 [00:07<00:00,  3.43it/s, val_loss=1.0340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30] Training Loss: 0.9884, Validation Loss: 1.2270\n",
      "Current learning rate: 0.000500\n",
      "No improvement for 2 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Train]: 100%|██████████| 81/81 [00:40<00:00,  2.00it/s, loss=0.9953]\n",
      "Epoch 10/30 [Val]: 100%|██████████| 27/27 [00:07<00:00,  3.45it/s, val_loss=1.0567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30] Training Loss: 0.9366, Validation Loss: 1.2470\n",
      "Current learning rate: 0.000500\n",
      "No improvement for 3 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Train]: 100%|██████████| 81/81 [00:41<00:00,  1.95it/s, loss=0.9240]\n",
      "Epoch 11/30 [Val]: 100%|██████████| 27/27 [00:07<00:00,  3.46it/s, val_loss=1.0127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30] Training Loss: 0.8919, Validation Loss: 1.2363\n",
      "Current learning rate: 0.000500\n",
      "No improvement for 4 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Train]: 100%|██████████| 81/81 [00:40<00:00,  2.00it/s, loss=1.0032]\n",
      "Epoch 12/30 [Val]: 100%|██████████| 27/27 [00:07<00:00,  3.48it/s, val_loss=1.0229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/30] Training Loss: 0.8589, Validation Loss: 1.1458\n",
      "Current learning rate: 0.000500\n",
      "New best validation loss: 1.1458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Train]: 100%|██████████| 81/81 [00:41<00:00,  1.97it/s, loss=0.7446]\n",
      "Epoch 13/30 [Val]: 100%|██████████| 27/27 [00:08<00:00,  3.36it/s, val_loss=1.0261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30] Training Loss: 0.8329, Validation Loss: 1.2075\n",
      "Current learning rate: 0.000500\n",
      "No improvement for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Train]: 100%|██████████| 81/81 [00:40<00:00,  2.00it/s, loss=0.6829]\n",
      "Epoch 14/30 [Val]: 100%|██████████| 27/27 [00:07<00:00,  3.38it/s, val_loss=1.0128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30] Training Loss: 0.7985, Validation Loss: 1.2032\n",
      "Current learning rate: 0.000500\n",
      "No improvement for 2 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [Train]: 100%|██████████| 81/81 [00:40<00:00,  2.01it/s, loss=0.7834]\n",
      "Epoch 15/30 [Val]: 100%|██████████| 27/27 [00:07<00:00,  3.48it/s, val_loss=1.0915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30] Training Loss: 0.7456, Validation Loss: 1.2325\n",
      "Current learning rate: 0.000250\n",
      "No improvement for 3 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Train]: 100%|██████████| 81/81 [00:41<00:00,  1.97it/s, loss=0.7626]\n",
      "Epoch 16/30 [Val]: 100%|██████████| 27/27 [00:07<00:00,  3.49it/s, val_loss=1.0686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30] Training Loss: 0.6627, Validation Loss: 1.2288\n",
      "Current learning rate: 0.000250\n",
      "No improvement for 4 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [Train]: 100%|██████████| 81/81 [00:40<00:00,  2.00it/s, loss=0.3935]\n",
      "Epoch 17/30 [Val]: 100%|██████████| 27/27 [00:07<00:00,  3.50it/s, val_loss=1.0733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30] Training Loss: 0.6217, Validation Loss: 1.2215\n",
      "Current learning rate: 0.000250\n",
      "No improvement for 5 epochs\n",
      "Early stopping after 17 epochs\n",
      "Training complete.\n",
      "Best validation loss: 1.1458\n",
      "Evaluating best model on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 27/27 [00:28<00:00,  1.06s/it, loss=1.0921]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Loss: 1.1111\n",
      "Overall Character Error Rate: 0.2820\n",
      "Overall Word Error Rate: 0.5125\n",
      "\n",
      "Per-Speaker Metrics:\n",
      "Speaker F01 (samples: 22)\n",
      "  - Character Error Rate: 0.5377\n",
      "  - Word Error Rate: 0.8422\n",
      "Speaker F03 (samples: 128)\n",
      "  - Character Error Rate: 0.4369\n",
      "  - Word Error Rate: 0.7304\n",
      "Speaker F04 (samples: 89)\n",
      "  - Character Error Rate: 0.2215\n",
      "  - Word Error Rate: 0.4067\n",
      "Speaker FC01 (samples: 42)\n",
      "  - Character Error Rate: 0.3103\n",
      "  - Word Error Rate: 0.6110\n",
      "Speaker FC02 (samples: 244)\n",
      "  - Character Error Rate: 0.1946\n",
      "  - Word Error Rate: 0.4468\n",
      "Speaker FC03 (samples: 192)\n",
      "  - Character Error Rate: 0.2699\n",
      "  - Word Error Rate: 0.4902\n",
      "Speaker M01 (samples: 13)\n",
      "  - Character Error Rate: 0.6692\n",
      "  - Word Error Rate: 0.9049\n",
      "Speaker M02 (samples: 79)\n",
      "  - Character Error Rate: 0.6209\n",
      "  - Word Error Rate: 0.9908\n",
      "Speaker M03 (samples: 96)\n",
      "  - Character Error Rate: 0.1470\n",
      "  - Word Error Rate: 0.2822\n",
      "Speaker M04 (samples: 89)\n",
      "  - Character Error Rate: 0.6290\n",
      "  - Word Error Rate: 0.9480\n",
      "Speaker M05 (samples: 19)\n",
      "  - Character Error Rate: 0.6769\n",
      "  - Word Error Rate: 0.9474\n",
      "Speaker MC01 (samples: 203)\n",
      "  - Character Error Rate: 0.1796\n",
      "  - Word Error Rate: 0.3776\n",
      "Speaker MC02 (samples: 135)\n",
      "  - Character Error Rate: 0.2355\n",
      "  - Word Error Rate: 0.4450\n",
      "Speaker MC03 (samples: 163)\n",
      "  - Character Error Rate: 0.1280\n",
      "  - Word Error Rate: 0.3056\n",
      "Speaker MC04 (samples: 205)\n",
      "  - Character Error Rate: 0.2692\n",
      "  - Word Error Rate: 0.4779\n",
      "Example 1 (Speaker: F04):\n",
      "Reference: 'knew'\n",
      "Prediction: 'knew'\n",
      "Example 2 (Speaker: FC01):\n",
      "Reference: 'whoop'\n",
      "Prediction: 'wope'\n",
      "Example 3 (Speaker: FC03):\n",
      "Reference: 'the family requests that flowers be omitted'\n",
      "Prediction: 'the family request that flowers be omitted'\n",
      "Example 4 (Speaker: MC02):\n",
      "Reference: 'shred'\n",
      "Prediction: 'shred'\n",
      "Example 5 (Speaker: FC03):\n",
      "Reference: 'i tell you it was wonderful'\n",
      "Prediction: 'i tell you it was wonderful'\n",
      "Example 6 (Speaker: MC04):\n",
      "Reference: 'bubble'\n",
      "Prediction: 'buble'\n",
      "Example 7 (Speaker: FC02):\n",
      "Reference: 'spade'\n",
      "Prediction: 'sade'\n",
      "Example 8 (Speaker: MC02):\n",
      "Reference: 'if you are losing water replace it immediately'\n",
      "Prediction: 'if youre losing water replace it immediately'\n",
      "Example 9 (Speaker: M03):\n",
      "Reference: 'slip'\n",
      "Prediction: 'slip'\n",
      "Example 10 (Speaker: F03):\n",
      "Reference: 'ride'\n",
      "Prediction: 'fidd'\n",
      "Example 11 (Speaker: FC02):\n",
      "Reference: 'bow as in bow and arrow'\n",
      "Prediction: 'bol'\n",
      "Example 12 (Speaker: F04):\n",
      "Reference: 'loyal'\n",
      "Prediction: 'loyal'\n",
      "Example 13 (Speaker: FC03):\n",
      "Reference: 'the hotel owner shrugged'\n",
      "Prediction: 'the hotel owner shrugged'\n",
      "Example 14 (Speaker: F04):\n",
      "Reference: 'sweet'\n",
      "Prediction: 'sweet'\n",
      "Example 15 (Speaker: MC01):\n",
      "Reference: 'the prospect of cutting back spending is an unpleasant one for any governor'\n",
      "Prediction: 'the prospect of cutting back spending is an unpleasant one for any governore'\n",
      "\n",
      "Evaluation results saved to evaluation_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "char_to_idx = {char: i+1 for i, char in enumerate(vocab)}\n",
    "idx_to_char = {i+1: char for i, char in enumerate(vocab)}\n",
    "vocab_size = len(vocab) + 1  # +1 for padding/blank token\n",
    "\n",
    "csv_file = \"/kaggle/working/mlpr-torgo-kaggle.csv\"\n",
    "full_dataset = TorgoASRDataset(csv_file)\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    range(len(full_dataset)), \n",
    "    test_size=0.2, \n",
    "    random_state=42 \n",
    ")\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    train_idx, \n",
    "    test_size=0.25,  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset = Subset(full_dataset, val_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "# Smaller batch size to prevent out of memory errors\n",
    "train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=24, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=24, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 1024\n",
    "hidden_dim = 256\n",
    "embedding_dim = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create improved encoder-decoder model with Bahdanau attention\n",
    "model = EncoderDecoderModel(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    encoder_layers=2,\n",
    "    dropout_rate=0.3,\n",
    "    max_decoding_length=50,  # Limit sequence length\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "best_val_loss = trainModel(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    char_to_idx,\n",
    "    num_epochs=30,\n",
    "    learning_rate=5e-4,\n",
    "    patience=5,\n",
    "    min_delta=0.01,\n",
    "    teacher_forcing_ratio=0.5\n",
    ")\n",
    "\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(\"Evaluating best model on test set:\")\n",
    "test_loss, predictions, references = evaluateModel(model, test_loader, char_to_idx, idx_to_char)\n",
    "\n",
    "# Display examples\n",
    "print(\"\\nExamples with improved Bahdanau attention:\")\n",
    "for i, (ref, pred) in enumerate(zip(references[:10], predictions[:10])):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Reference: '{ref}'\")\n",
    "    print(f\"Prediction: '{pred}'\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7146065,
     "sourceId": 11548203,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 923.614125,
   "end_time": "2025-05-05T10:05:48.272279",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-05T09:50:24.658154",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
